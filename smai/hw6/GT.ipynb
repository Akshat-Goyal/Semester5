{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization using Gradient Descent\n",
    "\n",
    "In this excercise, you are required to implement matrix factorization method, specifically [Non-Negative Matrix Factorization (NMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization), using gradient descent. You have to apply the matrix factorization to solve topic modeling. \n",
    "\n",
    "(Please refer to the tutorial on basics of topic modeling, LSI with SVD (Tutorial Set 4), for details on LSI)\n",
    "\n",
    "## Applying NMF to solve Topic Modeling\n",
    "Given a term document matrix $V$, NMF factorizes it into two matrix $W$ and $H$ with the property that all three documents have no negative elements.\n",
    "<img src=\"content/NMF.png\" alt=\"Non-negative matrix factorization\" style=\"width: 80%\">\n",
    "\n",
    "In Non-negative Matrix Factorization, a document-term matrix is approximately factorized into term-feature and feature-document matrices.\n",
    "\n",
    "$V = WH$ Matrix multiplication can be implemented as computing the column vectors of $V$ as linear combinations of the basis vectors (column vectors) in $W$ (or the topics discovered from the documents) using coefficients supplied by columns of $H$ (or the membership weights for the topics in each document). That is, each column of V can be computed as follows:\n",
    "$$ v_i = W h_i$$\n",
    "\n",
    "In what follows, we will first see an example of applying NMF by using [SKLearn NMF API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) for the task of topic modeling. Later you will be required to implement NMF using gradient descent.\n",
    "\n",
    "### Scikit-Learn implementation of NMF for topic modeling\n",
    "Given a set of multivariate  $n$-dimensional data vectors, they are put into an  $n\\times m$  matrix  $V$  as its columns, where  $m$  is the number of examples in the data set. This matrix  $V$  is approximately factorized into an  $n \\times t$  matrix  $W$  and an  $t \\times m$  matrix  $H$ , where  $t$  is generally less than  $n$  or  $m$ . Hence, this results in a compression of the original data matrix.\n",
    "\n",
    "In terms of topic modeling, the input document-term matrix  $V$  is factorized into a  $n \\times t$  document-topic matrix and a  $t \\times m$  topic-term matrix, where  $t$  is the number of topics produced. Similar to tutorial 4, we will be using 20 NewsFetch dataset for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute document features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english')\n",
    "vectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\n",
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute NMF using Scikit Learn library\n",
    "\n",
    "We will also write a function to display top 8 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words=8\n",
    "vocab = np.array(vectorizer_tfidf.get_feature_names())\n",
    "\n",
    "def show_topics(a):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in a])\n",
    "    \n",
    "    return [' '.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "d = 5 # num topics\n",
    "clf = decomposition.NMF(n_components=d, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = clf.fit_transform(vectors_tfidf)\n",
    "H1 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people don think just like objective say morality',\n",
       " 'graphics thanks files image file program windows know',\n",
       " 'space nasa launch shuttle orbit moon lunar earth',\n",
       " 'ico bobbe tek beauchaine bronx manhattan sank queens',\n",
       " 'god jesus bible believe christian atheism does belief']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972.7047125661534"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = np.linalg.norm(vectors_tfidf - W1 @ H1) ** 2 + 1 * (np.linalg.norm(W1) ** 2 + np.linalg.norm(H1) ** 2)\n",
    "cost /= 2\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF using SGD\n",
    "\n",
    "In stochastic gradient descent (SGD), we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it's much more efficient!\n",
    "\n",
    "### Applying SGD to NMF\n",
    "\n",
    "Goal: Decompose $V\\;(m \\times n)$ into\n",
    "$$ V \\approx HW$$\n",
    "where $W\\;(m \\times d)$ and $H\\;(d \\times n)$, $W,\\;H\\; \\geq \\;0$, and we've minimized the Frobenius norm of $V-WH$. The objective function can therefore be written as the following:\n",
    "$$\n",
    "\\min_{H \\geq 0, W \\geq 0} F(H,W) = \\frac{1}{2} ||V - HW||^{2} + \\frac{\\lambda}{2} \\left( ||H||^2 + ||W||^2 \\right)\n",
    "$$\n",
    "\n",
    "### Implementation of NMF using SGD (Excercise)\n",
    "__Approach:__ Given the objective function above, pick random positive $W$ & $H$, and then use SGD to optimize. \n",
    "\n",
    "(Note that the objective function is non-convex in nature, and is convex only if we consider $H$ and $W$ separately. You can directly write the gradient descent rule for the objective function presented above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at step 0 is 48592685.59721659 with lr=0.0001\n",
      "Cost at step 10 is 14100.299151302379 with lr=0.0001\n",
      "Cost at step 20 is 5859.600087972009 with lr=0.0001\n",
      "Cost at step 30 is 5686.410764032416 with lr=1e-05\n",
      "Cost at step 40 is 5683.919439744279 with lr=1.0000000000000002e-06\n",
      "Cost at step 50 is 5683.694798828767 with lr=1.0000000000000002e-07\n",
      "Cost at step 60 is 5683.672551118111 with lr=1.0000000000000004e-08\n",
      "Cost at step 70 is 5683.670328485033 with lr=1.0000000000000005e-09\n",
      "Cost at step 80 is 5683.67010624308 with lr=1.0000000000000006e-10\n",
      "Cost at step 90 is 5683.670084019101 with lr=1.0000000000000006e-11\n",
      "Cost 5683.67008201894\n",
      "(2034, 5) (5, 26576)\n"
     ]
    }
   ],
   "source": [
    "def grad_step(V, H, W, lamda=1, lr=1e-4):\n",
    "    cost = np.linalg.norm(V - H @ W) ** 2 + lamda * (np.linalg.norm(H) ** 2 + np.linalg.norm(W) ** 2)\n",
    "    cost /= 2\n",
    "    \n",
    "    gH = -V @ W.T + H @ W @ W.T + lamda * H\n",
    "    gW = -H.T @ V + H.T @ H @ W + lamda * W\n",
    "    \n",
    "    H -= lr * gH\n",
    "    W -= lr * gW\n",
    "    \n",
    "    return cost, H, W\n",
    "\n",
    "\n",
    "def get_lamda(cost_prev, cost_curr, lamda_0):\n",
    "    return lamda_0 * (1 - 1 / np.abs(cost_curr - cost_prev))\n",
    "\n",
    "def optimizer(V, H, W, steps = 100):\n",
    "    lr = 1e-4\n",
    "    factor = 0.1\n",
    "    threshold = 100\n",
    "    patience = 10\n",
    "    no_improve = 0\n",
    "    \n",
    "    cost_prev = 0\n",
    "    cost_curr = 1000000\n",
    "    \n",
    "    for i in range(steps):\n",
    "        cost, H, W = grad_step(V, H, W, lr=lr)\n",
    "        \n",
    "        cost_prev, cost_curr = cost_curr, cost\n",
    "        \n",
    "        if np.abs(cost_curr - cost_prev) <= threshold:\n",
    "            no_improve += 1\n",
    "            \n",
    "        if no_improve >= 10:\n",
    "            lr *= factor\n",
    "            no_improve = 0\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f\"Cost at step {i} is {cost} with lr={lr}\")\n",
    "    \n",
    "    return cost, H, W\n",
    "\n",
    "n, m = vectors_tfidf.shape\n",
    "t = 5\n",
    "H = np.abs(np.random.random((n, t)))\n",
    "W = np.abs(np.random.random((t, m)))\n",
    "\n",
    "cost, H, W = optimizer(vectors_tfidf, H, W)\n",
    "\n",
    "print(f\"Cost {cost}\")\n",
    "print(H.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 5) (5, 26576)\n"
     ]
    }
   ],
   "source": [
    "print(H.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0005026478854937\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.abs(vectors_tfidf - H @ W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transcendence doctoring leave 1993 qualit paramus gondola obey',\n",
       " 'dang bloc hinnom langley lava catastrophically grid specially',\n",
       " 'radiometer international walla leahy harassed inverse holler hugs',\n",
       " 'tips shove denoted engage gbt pontius mary fleming',\n",
       " 'ohainaut raondom derive programma 058 tss degraded pong']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_topics(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3682a2ec0517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_words' is not defined"
     ]
    }
   ],
   "source": [
    "print(topic_words.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
