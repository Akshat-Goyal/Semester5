{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll No: 2018101075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwGWNHs2xIsx"
   },
   "source": [
    "# Logistic Regression Excercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcaBeePgu_Tu"
   },
   "source": [
    "## Multi-class classification of MNIST using Logistic Regression\n",
    "\n",
    "The multi-class scenario for logistic regression is quite similar to the binary case, except that the label $y$ is now an integer in {1, ...., K} where $K$ is the number of classes. In this excercise you will be provided with handwritten digit images. Write the code and compute the test accuracy by training a logistic regression based classifier in (i) one-vs-one, and (ii) one-vs-all setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "\n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        #print('searching: %s'%nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        #print('searching: %s' % nb_path)\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "\n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "\n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "\n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "\n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "\n",
    "        #print('Found %d cells'%len(nb.cells))\n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "\n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "\n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "\n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "\n",
    "\n",
    "#  register the NotebookFinder with sys.meta_path\n",
    "print('running importer')\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9944,
     "status": "ok",
     "timestamp": 1596983406360,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "ManRVu7IsIjp",
    "outputId": "b48dd937-f2d5-4762-af1a-44fa03c44d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set();\n",
    "import pandas as pd\n",
    "from utils import plot_decision_boundary, get_accuracy, get_prediction\n",
    "from utils import plot_2D_input_datapoints, generate_gifs, sigmoid, normalize\n",
    "import math\n",
    "import gif\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9918,
     "status": "ok",
     "timestamp": 1596983406361,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "xbV2U06Cs45b"
   },
   "outputs": [],
   "source": [
    "# Let's initialize our weights using uniform distribution\n",
    "def weight_init_uniform_dist(X, y):\n",
    "  \n",
    "    np.random.seed(312)\n",
    "    n_samples, n_features = np.shape(X)\n",
    "    _, n_outputs = np.shape(y)\n",
    "\n",
    "    limit = 1 / math.sqrt(n_features)\n",
    "    weights = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
    "    weights[-1] = 0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36195,
     "status": "ok",
     "timestamp": 1596983432936,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "SAAbK03fLCR1"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "# One hot encoding of our output label vector y\n",
    "def one_hot(a):\n",
    "    b = np.zeros((a.size, a.max()+1))\n",
    "    b[np.arange(a.size), a] = 1\n",
    "    return b\n",
    "\n",
    "# Loading dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# One-hot encoding of target label, Y\n",
    "Y = digits.target\n",
    "Y = one_hot(Y)\n",
    "\n",
    "# Absorbing weight b of the hyperplane\n",
    "X = digits.data\n",
    "b_ones = np.ones((len(X), 1))\n",
    "X = np.hstack((X, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36177,
     "status": "ok",
     "timestamp": 1596983432939,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "yzdjTbEYLvPK",
    "outputId": "76ed5c87-3298-433d-cf76-d68026c46342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.reset_orig()\n",
    "\n",
    "plt.gray()\n",
    "plt.matshow(digits.images[10])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36148,
     "status": "ok",
     "timestamp": 1596983432942,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "3CIYTv4x65As",
    "outputId": "d9f59ee0-8392-4ba9-8cb8-11afc1669fb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:  (1308, 65)\n",
      "Validation dataset:  (188, 65)\n",
      "Test dataset:  (301, 65)\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into train, val, and test set.\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, shuffle=True, test_size = 0.167)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size = 0.12517)\n",
    "\n",
    "print(\"Training dataset: \", X_train.shape)\n",
    "print(\"Validation dataset: \", X_val.shape)\n",
    "print(\"Test dataset: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36132,
     "status": "ok",
     "timestamp": 1596983432945,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "d3NzkO4s68RX"
   },
   "outputs": [],
   "source": [
    "# Normalizing X_train and absorbing weight b of the hyperplane\n",
    "X_normalized_train = normalize(X_train[:, :64])\n",
    "\n",
    "b_ones = np.ones((len(X_normalized_train), 1))\n",
    "X_normalized_train = np.hstack((X_normalized_train, b_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36096,
     "status": "ok",
     "timestamp": 1596983432947,
     "user": {
      "displayName": "KARTIK RISHI BHARADWAJ 14BEE0070",
      "photoUrl": "",
      "userId": "12368401133146776355"
     },
     "user_tz": -330
    },
    "id": "pYrK4fK3iyyk",
    "outputId": "a73d5605-db30-4099-f72e-9cca8e2fe3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1308, 65)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_features, n_outputs):\n",
    "    const = 1 / np.sqrt(n_features)\n",
    "    W = np.random.randn(n_features, n_outputs) * const\n",
    "    W[-1, :] = 0\n",
    "    return W\n",
    "\n",
    "def cost_fun(Y, Y_pred):\n",
    "    Y_pred = np.where(Y == 1, Y_pred, 1 - Y_pred)\n",
    "    cost = -1 / Y.shape[0] * np.sum(np.log(Y_pred))\n",
    "    return cost\n",
    "\n",
    "def two_class_data(X_in, Y_in, k1, k2):\n",
    "    idx = np.where((Y_in[:, k1] == 1) | (Y_in[:, k2] == 1))\n",
    "    X = X_in[idx]\n",
    "    Y = Y_in[:, k1:k1+1][idx]\n",
    "    return X, Y\n",
    "\n",
    "def accuracy_onevsone(X, Y, W):\n",
    "    n_classes = int(np.sqrt(W.shape[1]))\n",
    "    y_pred = np.argmax(X @ W, axis = 1) // n_classes\n",
    "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
    "\n",
    "def accuracy_onevsall(X, Y, W):\n",
    "    y_pred = np.argmax(X @ W, axis = 1)\n",
    "    return np.sum(Y[np.arange(Y.shape[0]), y_pred]) / Y.shape[0]\n",
    "\n",
    "def train(X, Y, epochs=1000, lr=0.01):\n",
    "    n_samples, n_features = X.shape\n",
    "    W = init_weights(n_features, 1)\n",
    "    for epoch in range(epochs):\n",
    "        Y_pred = sigmoid(X @ W)\n",
    "        gradient = 1 / n_samples * X.T @ (Y - Y_pred)\n",
    "        W += lr * gradient\n",
    "        if epoch % (epochs // 5) == 0:\n",
    "            print(f\"Epoch {epoch}: train loss {cost_fun(Y, Y_pred)}\")\n",
    "    return W\n",
    "\n",
    "def onevsoneClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
    "    n_examples, n_features = X_train.shape\n",
    "    _, n_classes = Y_train.shape\n",
    "    \n",
    "    W = []\n",
    "    print(f\"Training one vs one classifier starts\")\n",
    "    for k1 in range(n_classes):\n",
    "        for k2 in range(n_classes):\n",
    "            print(f\"Training {k1} vs {k2} classifier\")\n",
    "            X, Y = two_class_data(X_train, Y_train, k1, k2)\n",
    "            W.append(train(X, Y, epochs, lr).ravel())\n",
    "    print(f\"Training one vs one classifier ends\")\n",
    "    return np.array(W).T\n",
    "\n",
    "def onevsallClassifier(X_train, Y_train, epochs=1000, lr=0.01):\n",
    "    n_examples, n_features = X_train.shape\n",
    "    _, n_classes = Y_train.shape\n",
    "    \n",
    "    W = []\n",
    "    print(f\"Training one vs all classifier starts\")\n",
    "    for k in range(n_classes):\n",
    "        print(f\"{k} vs all classifier starts\")\n",
    "        W.append(train(X_train, Y_train[:,k:k+1], epochs, lr).ravel())\n",
    "        print(f\"{k} vs all classifier ends\")\n",
    "    print(f\"Training one vs all classifier ends\")\n",
    "    return np.array(W).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one vs one classifier starts\n",
      "Training 0 vs 0 classifier\n",
      "Epoch 0: train loss 9.224423398869945\n",
      "Epoch 200: train loss 8.004118124338899e-09\n",
      "Epoch 400: train loss 8.0038170300514e-09\n",
      "Epoch 600: train loss 8.003515963519487e-09\n",
      "Epoch 800: train loss 8.003214932549411e-09\n",
      "Training 0 vs 1 classifier\n",
      "Epoch 0: train loss 4.773113302352283\n",
      "Epoch 200: train loss 0.00405735727816492\n",
      "Epoch 400: train loss 0.001959856480942599\n",
      "Epoch 600: train loss 0.0013001344116271092\n",
      "Epoch 800: train loss 0.0009771247351734644\n",
      "Training 0 vs 2 classifier\n",
      "Epoch 0: train loss 4.349808969188089\n",
      "Epoch 200: train loss 0.002728564825216295\n",
      "Epoch 400: train loss 0.0014272400923508726\n",
      "Epoch 600: train loss 0.000987461333328215\n",
      "Epoch 800: train loss 0.0007631281526712768\n",
      "Training 0 vs 3 classifier\n",
      "Epoch 0: train loss 0.9366382432684975\n",
      "Epoch 200: train loss 0.002914723156058065\n",
      "Epoch 400: train loss 0.0014886921469678713\n",
      "Epoch 600: train loss 0.0010094599183345905\n",
      "Epoch 800: train loss 0.000767623396885471\n",
      "Training 0 vs 4 classifier\n",
      "Epoch 0: train loss 0.488028823742435\n",
      "Epoch 200: train loss 0.005742373880139189\n",
      "Epoch 400: train loss 0.0027027424797914966\n",
      "Epoch 600: train loss 0.001807736064112372\n",
      "Epoch 800: train loss 0.0013724791368461041\n",
      "Training 0 vs 5 classifier\n",
      "Epoch 0: train loss 1.018006400395604\n",
      "Epoch 200: train loss 0.00427839297406005\n",
      "Epoch 400: train loss 0.0018933566903233607\n",
      "Epoch 600: train loss 0.001236118915751204\n",
      "Epoch 800: train loss 0.000927304163531359\n",
      "Training 0 vs 6 classifier\n",
      "Epoch 0: train loss 2.4351089478817167\n",
      "Epoch 200: train loss 0.0034700036572191044\n",
      "Epoch 400: train loss 0.0019191245195958064\n",
      "Epoch 600: train loss 0.0013847135591971691\n",
      "Epoch 800: train loss 0.0011021151163399551\n",
      "Training 0 vs 7 classifier\n",
      "Epoch 0: train loss 3.955854910997268\n",
      "Epoch 200: train loss 0.0023937081968610843\n",
      "Epoch 400: train loss 0.0011220145174279654\n",
      "Epoch 600: train loss 0.0007524581293024063\n",
      "Epoch 800: train loss 0.0005749629610243205\n",
      "Training 0 vs 8 classifier\n",
      "Epoch 0: train loss 2.6269250915427675\n",
      "Epoch 200: train loss 0.0028800402125380084\n",
      "Epoch 400: train loss 0.0014314644337737356\n",
      "Epoch 600: train loss 0.0009685374660889348\n",
      "Epoch 800: train loss 0.0007397860215919269\n",
      "Training 0 vs 9 classifier\n",
      "Epoch 0: train loss 6.218679560522699\n",
      "Epoch 200: train loss 0.0030973925829036507\n",
      "Epoch 400: train loss 0.0017572286836615178\n",
      "Epoch 600: train loss 0.0012573647858529471\n",
      "Epoch 800: train loss 0.0009883735166145732\n",
      "Training 1 vs 0 classifier\n",
      "Epoch 0: train loss 2.25727249206122\n",
      "Epoch 200: train loss 0.003034451993705498\n",
      "Epoch 400: train loss 0.001668347956111828\n",
      "Epoch 600: train loss 0.0011605174295670863\n",
      "Epoch 800: train loss 0.0008928555058568028\n",
      "Training 1 vs 1 classifier\n",
      "Epoch 0: train loss 1.108809351812654\n",
      "Epoch 200: train loss 0.0001428836936882986\n",
      "Epoch 400: train loss 7.771556544514097e-05\n",
      "Epoch 600: train loss 5.35514240166778e-05\n",
      "Epoch 800: train loss 4.0926504659186606e-05\n",
      "Training 1 vs 2 classifier\n",
      "Epoch 0: train loss 6.129134163476615\n",
      "Epoch 200: train loss 0.008655574179235154\n",
      "Epoch 400: train loss 0.004655949189804663\n",
      "Epoch 600: train loss 0.0032950353513445207\n",
      "Epoch 800: train loss 0.0025816013305694123\n",
      "Training 1 vs 3 classifier\n",
      "Epoch 0: train loss 4.171958412599843\n",
      "Epoch 200: train loss 0.007484209895971684\n",
      "Epoch 400: train loss 0.0035783188750499397\n",
      "Epoch 600: train loss 0.0023550601313114643\n",
      "Epoch 800: train loss 0.001760801031837881\n",
      "Training 1 vs 4 classifier\n",
      "Epoch 0: train loss 5.915301416184823\n",
      "Epoch 200: train loss 0.02277831261168395\n",
      "Epoch 400: train loss 0.013856608800440269\n",
      "Epoch 600: train loss 0.00972929142749373\n",
      "Epoch 800: train loss 0.007466865851036428\n",
      "Training 1 vs 5 classifier\n",
      "Epoch 0: train loss 1.0342173733249582\n",
      "Epoch 200: train loss 0.007621067434367205\n",
      "Epoch 400: train loss 0.003908754829748425\n",
      "Epoch 600: train loss 0.0026251578248391873\n",
      "Epoch 800: train loss 0.0019796925121213153\n",
      "Training 1 vs 6 classifier\n",
      "Epoch 0: train loss 3.668149838849502\n",
      "Epoch 200: train loss 0.006089789503632225\n",
      "Epoch 400: train loss 0.0038223942870498\n",
      "Epoch 600: train loss 0.002842467042288716\n",
      "Epoch 800: train loss 0.0022769325804577584\n",
      "Training 1 vs 7 classifier\n",
      "Epoch 0: train loss 4.921625884758342\n",
      "Epoch 200: train loss 0.007101102941484675\n",
      "Epoch 400: train loss 0.0035145789926076675\n",
      "Epoch 600: train loss 0.002350633642024666\n",
      "Epoch 800: train loss 0.0017731673590337218\n",
      "Training 1 vs 8 classifier\n",
      "Epoch 0: train loss 1.4278808891973123\n",
      "Epoch 200: train loss 0.05898738961050767\n",
      "Epoch 400: train loss 0.0363970497115675\n",
      "Epoch 600: train loss 0.025881792952352697\n",
      "Epoch 800: train loss 0.020116045110763942\n",
      "Training 1 vs 9 classifier\n",
      "Epoch 0: train loss 1.5833091929667167\n",
      "Epoch 200: train loss 0.013731283041346673\n",
      "Epoch 400: train loss 0.008214496889567553\n",
      "Epoch 600: train loss 0.005945530109100421\n",
      "Epoch 800: train loss 0.004682346695082626\n",
      "Training 2 vs 0 classifier\n",
      "Epoch 0: train loss 3.0997860017535754\n",
      "Epoch 200: train loss 0.0038980658259007155\n",
      "Epoch 400: train loss 0.001806517209953831\n",
      "Epoch 600: train loss 0.001193473068852623\n",
      "Epoch 800: train loss 0.0008977735347188912\n",
      "Training 2 vs 1 classifier\n",
      "Epoch 0: train loss 1.9713368274309382\n",
      "Epoch 200: train loss 0.008235003974854951\n",
      "Epoch 400: train loss 0.004661990163980797\n",
      "Epoch 600: train loss 0.003335191910167943\n",
      "Epoch 800: train loss 0.0026216660266086393\n",
      "Training 2 vs 2 classifier\n",
      "Epoch 0: train loss 0.8855754194464878\n",
      "Epoch 200: train loss 0.0001432195761267511\n",
      "Epoch 400: train loss 7.26861760241314e-05\n",
      "Epoch 600: train loss 4.877040761917101e-05\n",
      "Epoch 800: train loss 3.672272452181484e-05\n",
      "Training 2 vs 3 classifier\n",
      "Epoch 0: train loss 1.283128360690855\n",
      "Epoch 200: train loss 0.010687793077641887\n",
      "Epoch 400: train loss 0.005770682422729089\n",
      "Epoch 600: train loss 0.004046086664047039\n",
      "Epoch 800: train loss 0.003149655135276123\n",
      "Training 2 vs 4 classifier\n",
      "Epoch 0: train loss 4.84290428579105\n",
      "Epoch 200: train loss 0.0036272881215363667\n",
      "Epoch 400: train loss 0.0017351487290531273\n",
      "Epoch 600: train loss 0.0011442290039618463\n",
      "Epoch 800: train loss 0.0008567398670608304\n",
      "Training 2 vs 5 classifier\n",
      "Epoch 0: train loss 0.8071732236898312\n",
      "Epoch 200: train loss 0.0034636445720802878\n",
      "Epoch 400: train loss 0.0019127528582291397\n",
      "Epoch 600: train loss 0.0013546992928554\n",
      "Epoch 800: train loss 0.0010602976817014281\n",
      "Training 2 vs 6 classifier\n",
      "Epoch 0: train loss 0.8607895232802788\n",
      "Epoch 200: train loss 0.003152810750071747\n",
      "Epoch 400: train loss 0.0015329398492871234\n",
      "Epoch 600: train loss 0.0010306462114163361\n",
      "Epoch 800: train loss 0.0007852874878787478\n",
      "Training 2 vs 7 classifier\n",
      "Epoch 0: train loss 1.9093905877693558\n",
      "Epoch 200: train loss 0.003870260541746631\n",
      "Epoch 400: train loss 0.0020157475548188896\n",
      "Epoch 600: train loss 0.0013821049102039316\n",
      "Epoch 800: train loss 0.001059020830687651\n",
      "Training 2 vs 8 classifier\n",
      "Epoch 0: train loss 1.9535874392170736\n",
      "Epoch 200: train loss 0.007244477530170469\n",
      "Epoch 400: train loss 0.00403668169414967\n",
      "Epoch 600: train loss 0.002915034614548881\n",
      "Epoch 800: train loss 0.002318913686726277\n",
      "Training 2 vs 9 classifier\n",
      "Epoch 0: train loss 3.9658895800255887\n",
      "Epoch 200: train loss 0.004359671000623402\n",
      "Epoch 400: train loss 0.0021789407159685877\n",
      "Epoch 600: train loss 0.0014881085210985555\n",
      "Epoch 800: train loss 0.0011425795989718334\n",
      "Training 3 vs 0 classifier\n",
      "Epoch 0: train loss 0.8778771697618675\n",
      "Epoch 200: train loss 0.0030986190825988873\n",
      "Epoch 400: train loss 0.0015590641060692241\n",
      "Epoch 600: train loss 0.001050167342210873\n",
      "Epoch 800: train loss 0.0007947979376695398\n",
      "Training 3 vs 1 classifier\n",
      "Epoch 0: train loss 3.215382385202572\n",
      "Epoch 200: train loss 0.006597842767207617\n",
      "Epoch 400: train loss 0.003580056987676003\n",
      "Epoch 600: train loss 0.0024759882194714013\n",
      "Epoch 800: train loss 0.0018994090060952722\n",
      "Training 3 vs 2 classifier\n",
      "Epoch 0: train loss 3.9875671432623534\n",
      "Epoch 200: train loss 0.011659038067714277\n",
      "Epoch 400: train loss 0.0058329969396881025\n",
      "Epoch 600: train loss 0.003957018027883413\n",
      "Epoch 800: train loss 0.0030260183488890428\n",
      "Training 3 vs 3 classifier\n",
      "Epoch 0: train loss 1.42660442113764\n",
      "Epoch 200: train loss 7.695376780282506e-06\n",
      "Epoch 400: train loss 7.3590166838057475e-06\n",
      "Epoch 600: train loss 7.051002127739837e-06\n",
      "Epoch 800: train loss 6.767889148468359e-06\n",
      "Training 3 vs 4 classifier\n",
      "Epoch 0: train loss 2.3021329022238723\n",
      "Epoch 200: train loss 0.0027443667386538037\n",
      "Epoch 400: train loss 0.001321295583963611\n",
      "Epoch 600: train loss 0.0008825003758729232\n",
      "Epoch 800: train loss 0.0006683581294959316\n",
      "Training 3 vs 5 classifier\n",
      "Epoch 0: train loss 1.5326242823994878\n",
      "Epoch 200: train loss 0.010577435135682522\n",
      "Epoch 400: train loss 0.005921605721015467\n",
      "Epoch 600: train loss 0.0041701307199688824\n",
      "Epoch 800: train loss 0.0032383280346176572\n",
      "Training 3 vs 6 classifier\n",
      "Epoch 0: train loss 4.618578334124375\n",
      "Epoch 200: train loss 0.002606633328938648\n",
      "Epoch 400: train loss 0.0013834719079966762\n",
      "Epoch 600: train loss 0.0009507341292231931\n",
      "Epoch 800: train loss 0.0007277209465423363\n",
      "Training 3 vs 7 classifier\n",
      "Epoch 0: train loss 2.8981568694302062\n",
      "Epoch 200: train loss 0.008014239926581356\n",
      "Epoch 400: train loss 0.003726761102096946\n",
      "Epoch 600: train loss 0.0024383988333462167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800: train loss 0.0018313355670035514\n",
      "Training 3 vs 8 classifier\n",
      "Epoch 0: train loss 2.8379727496435776\n",
      "Epoch 200: train loss 0.021813794275720447\n",
      "Epoch 400: train loss 0.011958620099366322\n",
      "Epoch 600: train loss 0.008350359503638995\n",
      "Epoch 800: train loss 0.006450309550676362\n",
      "Training 3 vs 9 classifier\n",
      "Epoch 0: train loss 4.064718254043677\n",
      "Epoch 200: train loss 0.03046791045678567\n",
      "Epoch 400: train loss 0.01802099357506238\n",
      "Epoch 600: train loss 0.012949697920459287\n",
      "Epoch 800: train loss 0.010108520244187175\n",
      "Training 4 vs 0 classifier\n",
      "Epoch 0: train loss 0.7277343662526592\n",
      "Epoch 200: train loss 0.007204594859541454\n",
      "Epoch 400: train loss 0.003117042678081922\n",
      "Epoch 600: train loss 0.0020001272879567357\n",
      "Epoch 800: train loss 0.0014882136870617904\n",
      "Training 4 vs 1 classifier\n",
      "Epoch 0: train loss 3.803271409984284\n",
      "Epoch 200: train loss 0.022363877754119747\n",
      "Epoch 400: train loss 0.01268138562480091\n",
      "Epoch 600: train loss 0.008942155918371775\n",
      "Epoch 800: train loss 0.006941532735087818\n",
      "Training 4 vs 2 classifier\n",
      "Epoch 0: train loss 2.041122550789737\n",
      "Epoch 200: train loss 0.002179355275620467\n",
      "Epoch 400: train loss 0.0012043986658010525\n",
      "Epoch 600: train loss 0.0008554343317977915\n",
      "Epoch 800: train loss 0.0006698980566764166\n",
      "Training 4 vs 3 classifier\n",
      "Epoch 0: train loss 5.134095763734538\n",
      "Epoch 200: train loss 0.003631620845781389\n",
      "Epoch 400: train loss 0.0015880593750139203\n",
      "Epoch 600: train loss 0.0010260504978762274\n",
      "Epoch 800: train loss 0.0007617942520870105\n",
      "Training 4 vs 4 classifier\n",
      "Epoch 0: train loss 3.410706681572588\n",
      "Epoch 200: train loss 1.6145809955701942e-09\n",
      "Epoch 400: train loss 1.6145683684150224e-09\n",
      "Epoch 600: train loss 1.614555760852022e-09\n",
      "Epoch 800: train loss 1.614543141860255e-09\n",
      "Training 4 vs 5 classifier\n",
      "Epoch 0: train loss 4.550686818739949\n",
      "Epoch 200: train loss 0.006570186199744413\n",
      "Epoch 400: train loss 0.0035257590088616083\n",
      "Epoch 600: train loss 0.0024553880734337723\n",
      "Epoch 800: train loss 0.0018963504867773179\n",
      "Training 4 vs 6 classifier\n",
      "Epoch 0: train loss 13.199257848667411\n",
      "Epoch 200: train loss 0.005772127804766857\n",
      "Epoch 400: train loss 0.003194420279710703\n",
      "Epoch 600: train loss 0.0022624304484053443\n",
      "Epoch 800: train loss 0.0017686919995769704\n",
      "Training 4 vs 7 classifier\n",
      "Epoch 0: train loss 3.445919016430808\n",
      "Epoch 200: train loss 0.01185927289713631\n",
      "Epoch 400: train loss 0.005336006826691295\n",
      "Epoch 600: train loss 0.0034587430458303953\n",
      "Epoch 800: train loss 0.00257256709473254\n",
      "Training 4 vs 8 classifier\n",
      "Epoch 0: train loss 5.038422524176123\n",
      "Epoch 200: train loss 0.014575029445395913\n",
      "Epoch 400: train loss 0.007956811753722379\n",
      "Epoch 600: train loss 0.00551615126637818\n",
      "Epoch 800: train loss 0.004245616008546275\n",
      "Training 4 vs 9 classifier\n",
      "Epoch 0: train loss 1.7578531040968048\n",
      "Epoch 200: train loss 0.006325936015326512\n",
      "Epoch 400: train loss 0.00382613535182792\n",
      "Epoch 600: train loss 0.002775178223281294\n",
      "Epoch 800: train loss 0.0021880728671381778\n",
      "Training 5 vs 0 classifier\n",
      "Epoch 0: train loss 2.4169765282862214\n",
      "Epoch 200: train loss 0.0033681509695826788\n",
      "Epoch 400: train loss 0.001822703941723243\n",
      "Epoch 600: train loss 0.001271101936337438\n",
      "Epoch 800: train loss 0.0009826401365487921\n",
      "Training 5 vs 1 classifier\n",
      "Epoch 0: train loss 5.5927065338079816\n",
      "Epoch 200: train loss 0.0065627269237987906\n",
      "Epoch 400: train loss 0.003083739387117267\n",
      "Epoch 600: train loss 0.0020476550964044888\n",
      "Epoch 800: train loss 0.0015469457567669704\n",
      "Training 5 vs 2 classifier\n",
      "Epoch 0: train loss 0.8728415896286329\n",
      "Epoch 200: train loss 0.003972202590700049\n",
      "Epoch 400: train loss 0.002127728825466587\n",
      "Epoch 600: train loss 0.0014878255103601707\n",
      "Epoch 800: train loss 0.0011555941212358786\n",
      "Training 5 vs 3 classifier\n",
      "Epoch 0: train loss 0.5549105943172302\n",
      "Epoch 200: train loss 0.012117383377458489\n",
      "Epoch 400: train loss 0.006396178708146294\n",
      "Epoch 600: train loss 0.004413344351398701\n",
      "Epoch 800: train loss 0.0033925646212245772\n",
      "Training 5 vs 4 classifier\n",
      "Epoch 0: train loss 5.878323187407441\n",
      "Epoch 200: train loss 0.00604386721273462\n",
      "Epoch 400: train loss 0.003067024488143014\n",
      "Epoch 600: train loss 0.0020880195033487615\n",
      "Epoch 800: train loss 0.0015997378357767741\n",
      "Training 5 vs 5 classifier\n",
      "Epoch 0: train loss 0.783107789241759\n",
      "Epoch 200: train loss 6.145769396987293e-05\n",
      "Epoch 400: train loss 4.5275111426586724e-05\n",
      "Epoch 600: train loss 3.588236518436075e-05\n",
      "Epoch 800: train loss 2.9741114614969234e-05\n",
      "Training 5 vs 6 classifier\n",
      "Epoch 0: train loss 3.672485761150156\n",
      "Epoch 200: train loss 0.0045220464979146505\n",
      "Epoch 400: train loss 0.0027133797199743255\n",
      "Epoch 600: train loss 0.001969503173292775\n",
      "Epoch 800: train loss 0.0015527889398736802\n",
      "Training 5 vs 7 classifier\n",
      "Epoch 0: train loss 5.674269204147662\n",
      "Epoch 200: train loss 0.0071746933762865985\n",
      "Epoch 400: train loss 0.003656762866080025\n",
      "Epoch 600: train loss 0.002497798141586661\n",
      "Epoch 800: train loss 0.0019197974024258485\n",
      "Training 5 vs 8 classifier\n",
      "Epoch 0: train loss 5.412785599406837\n",
      "Epoch 200: train loss 0.009886349904724262\n",
      "Epoch 400: train loss 0.006035260700764367\n",
      "Epoch 600: train loss 0.004447271061439768\n",
      "Epoch 800: train loss 0.003547805439547862\n",
      "Training 5 vs 9 classifier\n",
      "Epoch 0: train loss 6.570539017793815\n",
      "Epoch 200: train loss 0.01504486954544867\n",
      "Epoch 400: train loss 0.008649406332416402\n",
      "Epoch 600: train loss 0.0060747777122767\n",
      "Epoch 800: train loss 0.004691026156115631\n",
      "Training 6 vs 0 classifier\n",
      "Epoch 0: train loss 6.01219128502917\n",
      "Epoch 200: train loss 0.005760753964495243\n",
      "Epoch 400: train loss 0.0030488638095408738\n",
      "Epoch 600: train loss 0.002072728458025299\n",
      "Epoch 800: train loss 0.0015692101171962528\n",
      "Training 6 vs 1 classifier\n",
      "Epoch 0: train loss 5.506648827015451\n",
      "Epoch 200: train loss 0.003487176930692757\n",
      "Epoch 400: train loss 0.002158197087974417\n",
      "Epoch 600: train loss 0.001646896499930555\n",
      "Epoch 800: train loss 0.001359868058480879\n",
      "Training 6 vs 2 classifier\n",
      "Epoch 0: train loss 4.419080768554016\n",
      "Epoch 200: train loss 0.0037547365445908727\n",
      "Epoch 400: train loss 0.0018389872720938562\n",
      "Epoch 600: train loss 0.0012519601411764983\n",
      "Epoch 800: train loss 0.0009597115491930418\n",
      "Training 6 vs 3 classifier\n",
      "Epoch 0: train loss 4.661600355326727\n",
      "Epoch 200: train loss 0.002809103192943156\n",
      "Epoch 400: train loss 0.0013582753528668294\n",
      "Epoch 600: train loss 0.0009149067004376695\n",
      "Epoch 800: train loss 0.0006969826313894113\n",
      "Training 6 vs 4 classifier\n",
      "Epoch 0: train loss 6.002335778169118\n",
      "Epoch 200: train loss 0.007305212018618443\n",
      "Epoch 400: train loss 0.003641464576540355\n",
      "Epoch 600: train loss 0.0024685579307977034\n",
      "Epoch 800: train loss 0.0018847353560677287\n",
      "Training 6 vs 5 classifier\n",
      "Epoch 0: train loss 0.8994529798390051\n",
      "Epoch 200: train loss 0.004217246154580403\n",
      "Epoch 400: train loss 0.0024692489935782755\n",
      "Epoch 600: train loss 0.0017858734373538566\n",
      "Epoch 800: train loss 0.0014104648732469079\n",
      "Training 6 vs 6 classifier\n",
      "Epoch 0: train loss 0.01333211490666698\n",
      "Epoch 200: train loss 0.00016851814236435526\n",
      "Epoch 400: train loss 8.518323155155972e-05\n",
      "Epoch 600: train loss 5.705977517785664e-05\n",
      "Epoch 800: train loss 4.2920444884852715e-05\n",
      "Training 6 vs 7 classifier\n",
      "Epoch 0: train loss 7.631520739559566\n",
      "Epoch 200: train loss 0.0014856681492938222\n",
      "Epoch 400: train loss 0.000783798654075572\n",
      "Epoch 600: train loss 0.0005389034103021752\n",
      "Epoch 800: train loss 0.0004131178813162195\n",
      "Training 6 vs 8 classifier\n",
      "Epoch 0: train loss 3.1126535951249195\n",
      "Epoch 200: train loss 0.00739595468002734\n",
      "Epoch 400: train loss 0.0038071440023532757\n",
      "Epoch 600: train loss 0.0026581184848010974\n",
      "Epoch 800: train loss 0.002061586348618366\n",
      "Training 6 vs 9 classifier\n",
      "Epoch 0: train loss 6.191602615935724\n",
      "Epoch 200: train loss 0.002635183499543993\n",
      "Epoch 400: train loss 0.0013356975862913122\n",
      "Epoch 600: train loss 0.0009024401689743618\n",
      "Epoch 800: train loss 0.0006844468151868818\n",
      "Training 7 vs 0 classifier\n",
      "Epoch 0: train loss 0.43988164650387906\n",
      "Epoch 200: train loss 0.0016893394933905088\n",
      "Epoch 400: train loss 0.0009371116759021862\n",
      "Epoch 600: train loss 0.0006674563409023929\n",
      "Epoch 800: train loss 0.0005250765561995812\n",
      "Training 7 vs 1 classifier\n",
      "Epoch 0: train loss 3.3698914143960357\n",
      "Epoch 200: train loss 0.008211072106289756\n",
      "Epoch 400: train loss 0.004162634809663315\n",
      "Epoch 600: train loss 0.0027810548479316657\n",
      "Epoch 800: train loss 0.0020921513416813776\n",
      "Training 7 vs 2 classifier\n",
      "Epoch 0: train loss 1.2230228235580514\n",
      "Epoch 200: train loss 0.0039788641245120035\n",
      "Epoch 400: train loss 0.0020176583273978704\n",
      "Epoch 600: train loss 0.0013743727184541384\n",
      "Epoch 800: train loss 0.0010505204191149121\n",
      "Training 7 vs 3 classifier\n",
      "Epoch 0: train loss 2.1146120935088013\n",
      "Epoch 200: train loss 0.007079553318691243\n",
      "Epoch 400: train loss 0.003516020530561853\n",
      "Epoch 600: train loss 0.0024177083196816047\n",
      "Epoch 800: train loss 0.001871306338341963\n",
      "Training 7 vs 4 classifier\n",
      "Epoch 0: train loss 4.382688062433339\n",
      "Epoch 200: train loss 0.008947423094652142\n",
      "Epoch 400: train loss 0.0044575321305222355\n",
      "Epoch 600: train loss 0.002995513688963688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800: train loss 0.002269320898565524\n",
      "Training 7 vs 5 classifier\n",
      "Epoch 0: train loss 4.86948688123844\n",
      "Epoch 200: train loss 0.00896992731049104\n",
      "Epoch 400: train loss 0.0045684080846560724\n",
      "Epoch 600: train loss 0.003081669815223007\n",
      "Epoch 800: train loss 0.002333107169455796\n",
      "Training 7 vs 6 classifier\n",
      "Epoch 0: train loss 0.606302434888378\n",
      "Epoch 200: train loss 0.001723110153446861\n",
      "Epoch 400: train loss 0.0008837713047459729\n",
      "Epoch 600: train loss 0.0005997007994388894\n",
      "Epoch 800: train loss 0.0004558647180325123\n",
      "Training 7 vs 7 classifier\n",
      "Epoch 0: train loss 5.332876491290743\n",
      "Epoch 200: train loss 2.418199860565663e-08\n",
      "Epoch 400: train loss 2.417926717802121e-08\n",
      "Epoch 600: train loss 2.4176536414729754e-08\n",
      "Epoch 800: train loss 2.4173806259375775e-08\n",
      "Training 7 vs 8 classifier\n",
      "Epoch 0: train loss 2.967899245087039\n",
      "Epoch 200: train loss 0.01248399305025782\n",
      "Epoch 400: train loss 0.006788056485229619\n",
      "Epoch 600: train loss 0.004732389396178273\n",
      "Epoch 800: train loss 0.0036575917471588417\n",
      "Training 7 vs 9 classifier\n",
      "Epoch 0: train loss 6.3267775963323585\n",
      "Epoch 200: train loss 0.010106850144445534\n",
      "Epoch 400: train loss 0.006306052609363928\n",
      "Epoch 600: train loss 0.00473732121337849\n",
      "Epoch 800: train loss 0.0038362269778672116\n",
      "Training 8 vs 0 classifier\n",
      "Epoch 0: train loss 2.1314280319205405\n",
      "Epoch 200: train loss 0.0024266862624602713\n",
      "Epoch 400: train loss 0.0013388114456285092\n",
      "Epoch 600: train loss 0.000947718577194417\n",
      "Epoch 800: train loss 0.0007427030151441459\n",
      "Training 8 vs 1 classifier\n",
      "Epoch 0: train loss 3.1967310353845795\n",
      "Epoch 200: train loss 0.04899926242915408\n",
      "Epoch 400: train loss 0.031421633928160955\n",
      "Epoch 600: train loss 0.023172007060500363\n",
      "Epoch 800: train loss 0.01843204236700072\n",
      "Training 8 vs 2 classifier\n",
      "Epoch 0: train loss 6.407046307571464\n",
      "Epoch 200: train loss 0.010390871164654647\n",
      "Epoch 400: train loss 0.0051798969103975774\n",
      "Epoch 600: train loss 0.0035478506178674846\n",
      "Epoch 800: train loss 0.002731387214763639\n",
      "Training 8 vs 3 classifier\n",
      "Epoch 0: train loss 0.9240068264473555\n",
      "Epoch 200: train loss 0.019637575678498016\n",
      "Epoch 400: train loss 0.010411990823334705\n",
      "Epoch 600: train loss 0.007265517848477222\n",
      "Epoch 800: train loss 0.005642949344945034\n",
      "Training 8 vs 4 classifier\n",
      "Epoch 0: train loss 3.749612991911834\n",
      "Epoch 200: train loss 0.021453705150709435\n",
      "Epoch 400: train loss 0.00944065105451478\n",
      "Epoch 600: train loss 0.005973425581378186\n",
      "Epoch 800: train loss 0.004415911500802148\n",
      "Training 8 vs 5 classifier\n",
      "Epoch 0: train loss 6.000559766924351\n",
      "Epoch 200: train loss 0.0118284560740105\n",
      "Epoch 400: train loss 0.006566065419513093\n",
      "Epoch 600: train loss 0.004588038482081011\n",
      "Epoch 800: train loss 0.0035457916255428336\n",
      "Training 8 vs 6 classifier\n",
      "Epoch 0: train loss 2.233932781654617\n",
      "Epoch 200: train loss 0.008810462862447053\n",
      "Epoch 400: train loss 0.004220434244073845\n",
      "Epoch 600: train loss 0.0028263709767566733\n",
      "Epoch 800: train loss 0.002141727879637812\n",
      "Training 8 vs 7 classifier\n",
      "Epoch 0: train loss 5.843396514802932\n",
      "Epoch 200: train loss 0.011914010155834385\n",
      "Epoch 400: train loss 0.006258509770274627\n",
      "Epoch 600: train loss 0.00434823586423112\n",
      "Epoch 800: train loss 0.0033640373270429303\n",
      "Training 8 vs 8 classifier\n",
      "Epoch 0: train loss 2.105432199437861\n",
      "Epoch 200: train loss 6.222047090490344e-08\n",
      "Epoch 400: train loss 6.219780923136626e-08\n",
      "Epoch 600: train loss 6.2175164196232e-08\n",
      "Epoch 800: train loss 6.215253575544412e-08\n",
      "Training 8 vs 9 classifier\n",
      "Epoch 0: train loss 1.5542688842973018\n",
      "Epoch 200: train loss 0.031797023495007355\n",
      "Epoch 400: train loss 0.018237572153763877\n",
      "Epoch 600: train loss 0.01306024919267779\n",
      "Epoch 800: train loss 0.010302113113788746\n",
      "Training 9 vs 0 classifier\n",
      "Epoch 0: train loss 0.8949899383311674\n",
      "Epoch 200: train loss 0.003821685380089949\n",
      "Epoch 400: train loss 0.001897813532338698\n",
      "Epoch 600: train loss 0.0012773457741500857\n",
      "Epoch 800: train loss 0.0009688247999051114\n",
      "Training 9 vs 1 classifier\n",
      "Epoch 0: train loss 3.7426219391602364\n",
      "Epoch 200: train loss 0.017135763379429302\n",
      "Epoch 400: train loss 0.008770806732037884\n",
      "Epoch 600: train loss 0.006053760127774242\n",
      "Epoch 800: train loss 0.004687512211117455\n",
      "Training 9 vs 2 classifier\n",
      "Epoch 0: train loss 1.032994366254434\n",
      "Epoch 200: train loss 0.005577126203977449\n",
      "Epoch 400: train loss 0.00282773253988842\n",
      "Epoch 600: train loss 0.0019122315740829952\n",
      "Epoch 800: train loss 0.0014515342489308878\n",
      "Training 9 vs 3 classifier\n",
      "Epoch 0: train loss 3.023028384300682\n",
      "Epoch 200: train loss 0.024182379338702424\n",
      "Epoch 400: train loss 0.014754411928801442\n",
      "Epoch 600: train loss 0.010804035471474195\n",
      "Epoch 800: train loss 0.00858685849457826\n",
      "Training 9 vs 4 classifier\n",
      "Epoch 0: train loss 3.892003857120503\n",
      "Epoch 200: train loss 0.012206892408296385\n",
      "Epoch 400: train loss 0.005341753595694883\n",
      "Epoch 600: train loss 0.0034280534103696293\n",
      "Epoch 800: train loss 0.002534475644567984\n",
      "Training 9 vs 5 classifier\n",
      "Epoch 0: train loss 3.401507077758356\n",
      "Epoch 200: train loss 0.01200446324557764\n",
      "Epoch 400: train loss 0.007072356050456964\n",
      "Epoch 600: train loss 0.005149651826474185\n",
      "Epoch 800: train loss 0.004084211175488635\n",
      "Training 9 vs 6 classifier\n",
      "Epoch 0: train loss 4.237817936239709\n",
      "Epoch 200: train loss 0.003515754232205\n",
      "Epoch 400: train loss 0.0012299778016801861\n",
      "Epoch 600: train loss 0.0007654450127742253\n",
      "Epoch 800: train loss 0.0005658425070934944\n",
      "Training 9 vs 7 classifier\n",
      "Epoch 0: train loss 3.5524542921905278\n",
      "Epoch 200: train loss 0.01368005847817783\n",
      "Epoch 400: train loss 0.007598114687281889\n",
      "Epoch 600: train loss 0.0053787365835482855\n",
      "Epoch 800: train loss 0.00419592183961796\n",
      "Training 9 vs 8 classifier\n",
      "Epoch 0: train loss 0.700644756238336\n",
      "Epoch 200: train loss 0.030444931552062304\n",
      "Epoch 400: train loss 0.017748438647961808\n",
      "Epoch 600: train loss 0.012987095730080724\n",
      "Epoch 800: train loss 0.010360486803895527\n",
      "Training 9 vs 9 classifier\n",
      "Epoch 0: train loss 4.8652316133136395\n",
      "Epoch 200: train loss 5.360796427176311e-09\n",
      "Epoch 400: train loss 5.360654018269059e-09\n",
      "Epoch 600: train loss 5.360511615751581e-09\n",
      "Epoch 800: train loss 5.360369216428992e-09\n",
      "Training one vs one classifier ends\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(65, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_onevsone = onevsoneClassifier(X_train, Y_train)\n",
    "W_onevsone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training one vs all classifier starts\n",
      "0 vs all classifier starts\n",
      "Epoch 0: train loss 1.664570350040058\n",
      "Epoch 200: train loss 0.012751109634733127\n",
      "Epoch 400: train loss 0.007167189652143117\n",
      "Epoch 600: train loss 0.005139368380181322\n",
      "Epoch 800: train loss 0.00408826344654866\n",
      "0 vs all classifier ends\n",
      "1 vs all classifier starts\n",
      "Epoch 0: train loss 2.29764698407135\n",
      "Epoch 200: train loss 0.067179540840804\n",
      "Epoch 400: train loss 0.056895696498393704\n",
      "Epoch 600: train loss 0.05187500972137221\n",
      "Epoch 800: train loss 0.04867523220692286\n",
      "1 vs all classifier ends\n",
      "2 vs all classifier starts\n",
      "Epoch 0: train loss 2.625146099909089\n",
      "Epoch 200: train loss 0.027874662914575188\n",
      "Epoch 400: train loss 0.01660497012389972\n",
      "Epoch 600: train loss 0.012075848336271124\n",
      "Epoch 800: train loss 0.009582452444725924\n",
      "2 vs all classifier ends\n",
      "3 vs all classifier starts\n",
      "Epoch 0: train loss 1.3264571479382945\n",
      "Epoch 200: train loss 0.05698494582767506\n",
      "Epoch 400: train loss 0.041583529313282505\n",
      "Epoch 600: train loss 0.03513461746767037\n",
      "Epoch 800: train loss 0.03157187744450594\n",
      "3 vs all classifier ends\n",
      "4 vs all classifier starts\n",
      "Epoch 0: train loss 4.144098825465042\n",
      "Epoch 200: train loss 0.020644033023261874\n",
      "Epoch 400: train loss 0.014456427380758023\n",
      "Epoch 600: train loss 0.011583806203663312\n",
      "Epoch 800: train loss 0.009819299921122139\n",
      "4 vs all classifier ends\n",
      "5 vs all classifier starts\n",
      "Epoch 0: train loss 11.667555330365538\n",
      "Epoch 200: train loss 0.02640768171012352\n",
      "Epoch 400: train loss 0.017990246164434127\n",
      "Epoch 600: train loss 0.01429236581553648\n",
      "Epoch 800: train loss 0.012082489641175495\n",
      "5 vs all classifier ends\n",
      "6 vs all classifier starts\n",
      "Epoch 0: train loss 4.874552970396107\n",
      "Epoch 200: train loss 0.023992763321476658\n",
      "Epoch 400: train loss 0.015986342276358048\n",
      "Epoch 600: train loss 0.012662088752560488\n",
      "Epoch 800: train loss 0.010723747463854013\n",
      "6 vs all classifier ends\n",
      "7 vs all classifier starts\n",
      "Epoch 0: train loss 1.0852939372621535\n",
      "Epoch 200: train loss 0.029321447231850508\n",
      "Epoch 400: train loss 0.01869966725784806\n",
      "Epoch 600: train loss 0.014754849988906296\n",
      "Epoch 800: train loss 0.01260049167126755\n",
      "7 vs all classifier ends\n",
      "8 vs all classifier starts\n",
      "Epoch 0: train loss 7.6441208514304\n",
      "Epoch 200: train loss 0.12714600441984994\n",
      "Epoch 400: train loss 0.10668982348729707\n",
      "Epoch 600: train loss 0.09953206762107332\n",
      "Epoch 800: train loss 0.09572283331022303\n",
      "8 vs all classifier ends\n",
      "9 vs all classifier starts\n",
      "Epoch 0: train loss 1.5960434845339724\n",
      "Epoch 200: train loss 0.0973811570346086\n",
      "Epoch 400: train loss 0.06291467572553042\n",
      "Epoch 600: train loss 0.051318005857755006\n",
      "Epoch 800: train loss 0.04560897352566035\n",
      "9 vs all classifier ends\n",
      "Training one vs all classifier ends\n"
     ]
    }
   ],
   "source": [
    "W_onevsall = onevsallClassifier(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.18577981651376146\n",
      "Validation accuracy: 0.18085106382978725\n",
      "Test accuracy: 0.15946843853820597\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_onevsone(X_train, Y_train, W_onevsone)}\")\n",
    "print(f\"Validation accuracy: {accuracy_onevsone(X_val, Y_val, W_onevsone)}\")\n",
    "print(f\"Test accuracy: {accuracy_onevsone(X_test, Y_test, W_onevsone)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs All Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.981651376146789\n",
      "Validation accuracy: 0.9521276595744681\n",
      "Test accuracy: 0.9568106312292359\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_onevsall(X_train, Y_train, W_onevsall)}\")\n",
    "print(f\"Validation accuracy: {accuracy_onevsall(X_val, Y_val, W_onevsall)}\")\n",
    "print(f\"Test accuracy: {accuracy_onevsall(X_test, Y_test, W_onevsall)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LogisticRegression_draft4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
