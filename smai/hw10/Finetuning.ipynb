{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ba65dba8ddc484a9c1b7201b33f144a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b469719bd7840579d408295a0af6b4e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c10f045eefcf4fe7bd18c727acc06b38",
              "IPY_MODEL_cb87bb955ece4b68843fcafdd2068952"
            ]
          }
        },
        "1b469719bd7840579d408295a0af6b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c10f045eefcf4fe7bd18c727acc06b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e508dcad72174652a5bc79525e8ac857",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244418560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244418560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6042282d106d404aafcdceab0fd94388"
          }
        },
        "cb87bb955ece4b68843fcafdd2068952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00805a54f1494165a1f28a3903abba5d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [09:14&lt;00:00, 441kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eae74e1b43ae473c91c8360746852038"
          }
        },
        "e508dcad72174652a5bc79525e8ac857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6042282d106d404aafcdceab0fd94388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00805a54f1494165a1f28a3903abba5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eae74e1b43ae473c91c8360746852038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKHm7IdXiM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069e698b-3dd6-4c22-e510-ac6520df19b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ISIieY7P6HI"
      },
      "source": [
        "# Finetuning\n",
        "\n",
        "We saw in the earlier notebook that training a deep neural network requires a lot of training examples. When such a deep neural network is trained, the initial layers learn generic set of features and the later layers can be thought of as classifying those generic features. Hence, one can use pretrained features from networks that are trained on a big dataset to a new related problem where data is scarce.\n",
        "\n",
        "In this tutorial, we will see we will learn how to train the network using transfer learning to further improve these features to our task. We will use alexnet architecture, that is trained on 1000-class ImageNet dataset. \n",
        "\n",
        "In this document we will perform transfer learning by two types of finetuning. First, we start with a pretrained model and update all of the modelâ€™s parameters for our new task, in essence retraining the whole model. Second, we start with a pretrained model and only update the final layer weights from which we derive predictions. In this we use the pretrained CNN as a fixed feature-extractor, and only change the output layer.\n",
        "\n",
        "In general both transfer learning methods follow the same few steps:\n",
        "- Initialize the pretrained model\n",
        "- Reshape the final layer(s) to have the same number of outputs as the number of classes in the new dataset\n",
        "- Define for the optimization algorithm which parameters we want to update during training\n",
        "- Run the training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0wXG-onPh46"
      },
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQRJH1CWUC6g"
      },
      "source": [
        "## Input\n",
        "\n",
        "Here are all of the parameters to change for the run. We will use the hymenoptera_data dataset which can be downloaded here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>. This dataset contains two classes, bees and ants, and is structured such that we can use the ImageFolder <https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder> dataset, rather than writing our own custom dataset. Download the data and set the data_dir input to the root directory of the dataset. We will be using AlexNet as our deep neural network model that we wish to finetune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RAwc3tJRaO9"
      },
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms \n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/hymenoptera_data\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"alexnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model, \n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S2Kt7c5YwBD"
      },
      "source": [
        "## Helper functions\n",
        "\n",
        "Before we write the code for adjusting the models, lets define a few helper functions.\n",
        "\n",
        "Model Training and Validation Code\n",
        "\n",
        "```\n",
        "The ``train_model`` function handles the training and validation of a\n",
        "given model. As input, it takes a PyTorch model, a dictionary of\n",
        "dataloaders, a loss function, an optimizer, a specified number of epochs\n",
        "to train and validate for, and a boolean flag for when the model is an\n",
        "Inception model. It also keeps track of the best performing model (in terms of validation \n",
        "accuracy), and at the end of training returns the best performing model. After each epoch, \n",
        "the training and validation accuracies are printed.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1jt_DoYUsSb"
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kGf8zUTYziy"
      },
      "source": [
        "Set Model Parametersâ€™ .requires_grad attribute\n",
        "\n",
        "```\n",
        "This helper function sets the ``.requires_grad`` attribute of the\n",
        "parameters in the model to False when we are feature extracting. By\n",
        "default, when we load a pretrained model all of the parameters have\n",
        "``.requires_grad=True``, which is fine if we are training from scratch\n",
        "or finetuning. However, if we are feature extracting and only want to\n",
        "compute gradients for the newly initialized layer then we want all of\n",
        "the other parameters to not require gradients. This will make more sense\n",
        "later.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpbi00SUVeus"
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0iR9wRIZYfJ"
      },
      "source": [
        "## Initialize and reshape the network\n",
        "\n",
        "Now to the most interesting part. Here is where we handle the reshaping of the network. Note, this is not an automatic procedure and is unique to a model. Recall, the final layer of a CNN model, which is often times an FC layer, has the same number of nodes as the number of output classes in the dataset. Since all of the models have been pretrained on Imagenet, they all have output layers of size 1000, one node for each class. The goal here is to reshape the last layer to have the same number of inputs as before, AND to have the same number of outputs as the number of classes in the dataset. In the following sections we will discuss how to alter the architecture of each model individually. But first, there is one important detail regarding the difference between the two modes of fine-tuning\n",
        "\n",
        "When feature extracting, we only want to update the parameters of the last layer, or in other words, we only want to update the parameters for the layer(s) we are reshaping. Therefore, we do not need to compute the gradients of the parameters that we are not changing, so for efficiency we set the .`requires_grad` attribute to `False`. This is important because by default, this attribute is set to `True`. Then, when we initialize the new layer and by default the new parameters have `.requires_grad=True` so only the new layerâ€™s parameters will be updated. When we are finetuning all the layers, we can leave all of the `.required_grad`â€™s set to the default of `True`.\n",
        "\n",
        "Now, we will specifically look at Alexnet. Alexnet was introduced in the paper ImageNet Classification with Deep Convolutional Neural Networks <https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf>__ and was the first very successful CNN on the ImageNet dataset. When we print the model architecture, we see the model output comes from the 6th layer of the classifier\n",
        "\n",
        "::\n",
        "```\n",
        "(classifier): Sequential( ... (6): Linear(in_features=4096, out_features=1000, bias=True) )\n",
        "```\n",
        "To use the model with our dataset we reinitialize this layer as\n",
        "\n",
        "::\n",
        "\n",
        "`model.classifier[6] = nn.Linear(4096,num_classes)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsXjk7qjVfM2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536,
          "referenced_widgets": [
            "0ba65dba8ddc484a9c1b7201b33f144a",
            "1b469719bd7840579d408295a0af6b4e",
            "c10f045eefcf4fe7bd18c727acc06b38",
            "cb87bb955ece4b68843fcafdd2068952",
            "e508dcad72174652a5bc79525e8ac857",
            "6042282d106d404aafcdceab0fd94388",
            "00805a54f1494165a1f28a3903abba5d",
            "eae74e1b43ae473c91c8360746852038"
          ]
        },
        "outputId": "d142a37a-86d1-43dd-beae-cba33a05c849"
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    \"\"\" Alexnet\n",
        "    \"\"\"\n",
        "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "    set_parameter_requires_grad(model_ft, feature_extract)\n",
        "    num_ftrs = model_ft.classifier[6].in_features\n",
        "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "    input_size = 224\n",
        "    \n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ba65dba8ddc484a9c1b7201b33f144a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm2Fw1yMaxa8"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Now that we know what the input size must be, we can initialize the data transforms, image datasets, and the dataloaders. Notice, the models were pretrained with the hard-coded normalization values, as described here <https://pytorch.org/docs/master/torchvision/models.html>__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaUfoV80WWqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc29d739-5fd2-4474-dde3-6f0a9c407dd1"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z18XnbZaa6-s"
      },
      "source": [
        "## Create the Optimizer\n",
        "\n",
        "Now that the model structure is correct, the final step for finetuning\n",
        "and feature extracting is to create an optimizer that only updates the\n",
        "desired parameters. Recall that after loading the pretrained model, but\n",
        "before reshaping, if ``feature_extract=True`` we manually set all of the\n",
        "parameterâ€™s ``.requires_grad`` attributes to False. Then the\n",
        "reinitialized layerâ€™s parameters have ``.requires_grad=True`` by\n",
        "default. So now we know that all parameters that have\n",
        "``.requires_grad=True`` should be optimized. Next, we make a list of such\n",
        "parameters and input this list to the SGD algorithm constructor.\n",
        "\n",
        "To verify this, check out the printed parameters to learn. When\n",
        "finetuning, this list should be long and include all of the model\n",
        "parameters. However, when feature extracting this list should be short\n",
        "and only include the weights and biases of the reshaped layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9APZHBKIWrEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90aa85ed-2d1e-4027-8b39-7bdcf205533d"
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are \n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2E4_ifbK9q"
      },
      "source": [
        "## Run Training and Validation Step\n",
        "\n",
        "Finally, the last step is to setup the loss for the model, then run the\n",
        "training and validation function for the set number of epochs. Notice,\n",
        "depending on the number of epochs this step may take a while on a CPU.\n",
        "Also, the default learning rate is not optimal for all of the models, so\n",
        "to achieve maximum accuracy it would be necessary to tune for each model\n",
        "separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoKW0SiDYJb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c8069d-59b2-4a57-dff2-58b3e0a6af80"
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 0.5769 Acc: 0.7623\n",
            "val Loss: 0.3130 Acc: 0.8889\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.3536 Acc: 0.8934\n",
            "val Loss: 0.3285 Acc: 0.9085\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.1691 Acc: 0.9262\n",
            "val Loss: 0.4107 Acc: 0.9020\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.2943 Acc: 0.9180\n",
            "val Loss: 0.4814 Acc: 0.9085\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.3329 Acc: 0.9057\n",
            "val Loss: 0.4670 Acc: 0.8889\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.3136 Acc: 0.9016\n",
            "val Loss: 0.5571 Acc: 0.8954\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.2266 Acc: 0.9262\n",
            "val Loss: 0.4231 Acc: 0.9020\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.2168 Acc: 0.9139\n",
            "val Loss: 0.4628 Acc: 0.8954\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.2200 Acc: 0.9262\n",
            "val Loss: 0.4538 Acc: 0.9150\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.1774 Acc: 0.9467\n",
            "val Loss: 0.3765 Acc: 0.9150\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.1394 Acc: 0.9508\n",
            "val Loss: 0.3471 Acc: 0.9020\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.1620 Acc: 0.9549\n",
            "val Loss: 0.4873 Acc: 0.8889\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.2225 Acc: 0.9344\n",
            "val Loss: 0.6058 Acc: 0.9020\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.1364 Acc: 0.9631\n",
            "val Loss: 0.5404 Acc: 0.9020\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.1625 Acc: 0.9590\n",
            "val Loss: 0.4712 Acc: 0.8954\n",
            "\n",
            "Training complete in 1m 46s\n",
            "Best val Acc: 0.915033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZkHFltAbST1"
      },
      "source": [
        "## Comparison with Model Trained from Scratch\n",
        "\n",
        "Just for fun, lets see how the model learns if we do not use transfer\n",
        "learning. The performance of finetuning vs.Â feature extracting depends\n",
        "largely on the dataset but in general both transfer learning methods\n",
        "produce favorable results in terms of training time and overall accuracy\n",
        "versus a model trained from scratch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBuukDofYU9g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b43c3a1d-1fa1-4aaf-d85e-16cff1e617db"
      },
      "source": [
        "# Initialize the non-pretrained version of the model used for this run\n",
        "scratch_model,_ = initialize_model(model_name, num_classes, feature_extract=False, use_pretrained=False)\n",
        "scratch_model = scratch_model.to(device)\n",
        "scratch_optimizer = optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "scratch_criterion = nn.CrossEntropyLoss()\n",
        "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=num_epochs)\n",
        "\n",
        "# Plot the training curves of validation accuracy vs. number \n",
        "#  of training epochs for the transfer learning method and\n",
        "#  the model trained from scratch\n",
        "ohist = []\n",
        "shist = []\n",
        "\n",
        "ohist = [h.cpu().numpy() for h in hist]\n",
        "shist = [h.cpu().numpy() for h in scratch_hist]\n",
        "\n",
        "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
        "plt.plot(range(1,num_epochs+1),shist,label=\"Scratch\")\n",
        "plt.ylim((0,1.))\n",
        "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "train Loss: 0.6939 Acc: 0.4959\n",
            "val Loss: 0.6944 Acc: 0.4575\n",
            "\n",
            "Epoch 1/14\n",
            "----------\n",
            "train Loss: 0.6938 Acc: 0.4631\n",
            "val Loss: 0.6930 Acc: 0.5098\n",
            "\n",
            "Epoch 2/14\n",
            "----------\n",
            "train Loss: 0.6933 Acc: 0.5000\n",
            "val Loss: 0.6924 Acc: 0.5425\n",
            "\n",
            "Epoch 3/14\n",
            "----------\n",
            "train Loss: 0.6936 Acc: 0.4877\n",
            "val Loss: 0.6917 Acc: 0.5425\n",
            "\n",
            "Epoch 4/14\n",
            "----------\n",
            "train Loss: 0.6928 Acc: 0.5492\n",
            "val Loss: 0.6922 Acc: 0.5882\n",
            "\n",
            "Epoch 5/14\n",
            "----------\n",
            "train Loss: 0.6928 Acc: 0.5000\n",
            "val Loss: 0.6924 Acc: 0.6275\n",
            "\n",
            "Epoch 6/14\n",
            "----------\n",
            "train Loss: 0.6926 Acc: 0.5287\n",
            "val Loss: 0.6919 Acc: 0.6601\n",
            "\n",
            "Epoch 7/14\n",
            "----------\n",
            "train Loss: 0.6925 Acc: 0.5287\n",
            "val Loss: 0.6926 Acc: 0.4706\n",
            "\n",
            "Epoch 8/14\n",
            "----------\n",
            "train Loss: 0.6928 Acc: 0.5082\n",
            "val Loss: 0.6919 Acc: 0.6144\n",
            "\n",
            "Epoch 9/14\n",
            "----------\n",
            "train Loss: 0.6927 Acc: 0.5533\n",
            "val Loss: 0.6914 Acc: 0.6536\n",
            "\n",
            "Epoch 10/14\n",
            "----------\n",
            "train Loss: 0.6923 Acc: 0.5820\n",
            "val Loss: 0.6909 Acc: 0.6405\n",
            "\n",
            "Epoch 11/14\n",
            "----------\n",
            "train Loss: 0.6928 Acc: 0.5287\n",
            "val Loss: 0.6893 Acc: 0.5556\n",
            "\n",
            "Epoch 12/14\n",
            "----------\n",
            "train Loss: 0.6915 Acc: 0.5615\n",
            "val Loss: 0.6888 Acc: 0.6078\n",
            "\n",
            "Epoch 13/14\n",
            "----------\n",
            "train Loss: 0.6909 Acc: 0.5779\n",
            "val Loss: 0.6886 Acc: 0.6471\n",
            "\n",
            "Epoch 14/14\n",
            "----------\n",
            "train Loss: 0.6915 Acc: 0.5205\n",
            "val Loss: 0.6864 Acc: 0.5556\n",
            "\n",
            "Training complete in 0m 59s\n",
            "Best val Acc: 0.660131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClxJRfdIb0aj"
      },
      "source": [
        "## Excercise\n",
        "Try finetuning the entire pretrained model. Report the best accuracy achieved. Does this model work better or worse than only fine-tuning the final layer? Report your result with and without using weight decay for both approaches.\n",
        "\n",
        "## Excercise (optional)\n",
        "\n",
        "Try changing the number of layers to finetune. You can choose to finetune the train only a two of the last fully connected layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7S6CiosrrJV"
      },
      "source": [
        "### Your code here ###"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}